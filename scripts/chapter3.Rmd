```{r, warning = FALSE, message=FALSE}
library(foreign)
library(arm)
kidiq <- read.dta("../data/child.iq/kidiq.dta")
attach(kidiq)
```

# One predictor



## Binary predictor

$kid.score = 78 + 12 * mom.hs + error$

deterministic part only i.e. the expected value:

$\widehat{kid.score} = 78 + 12 * mom.hs$

= "summary of differences between average scores". So the model is a comparison of averages, the average for those who's mothers completed high school and those who didn't.

```{r, echo = FALSE}
## Plot Figure 3.1
kidscore.jitter <- jitter(kid_score)

jitter.binary <- function(a, jitt=.05){
   ifelse (a==0, runif(434, 0, jitt), runif (434, 1-jitt, 1))
}

jitter.mom_hs <- jitter.binary(mom_hs)

fit.0 <- lm (kid_score ~ mom_hs)
display(fit.0)

plot(jitter.mom_hs,kidscore.jitter, xlab="Mother completed high school", 
  ylab="Child test score",pch=20, xaxt="n", yaxt="n")
axis (1)
axis (2, c(20,60,100,140))
abline (fit.0)
```

## Continuous predictor

$kid.score = 26 + 0.6 * mom.iq + error$

Now each point on the line can be seen as the average score of the kids who's mothers have that IQ.

```{r, echo = FALSE}

## Plot Figure 3.2
fit.1 <- lm (kid_score ~ mom_iq)
display(fit.1)
plot(mom_iq,kid_score, xlab="Mother IQ score", 
  ylab="Child test score",pch=20, xaxt="n", yaxt="n")
axis (1, c(80,100,120,140))
axis (2, c(20,60,100,140))
abline (fit.1)
```

# Multiple predictors

$kid.score = 26 + 6*mom.hs + 0.6 * mom.iq + error$

Here interpretation is kind of reasonable, we canlook at two parellel regression lines for kids with moms with and without hs. So you can think of one predictor being held constant, but this is not always possible (e.g. if one predictor is an interaction it doesn't make sense).


```{r, echo = FALSE}

## Plot Figure 3.2
fit.2 <- lm (kid_score ~ mom_hs + mom_iq)
plot(mom_iq,kid_score, xlab="Mother IQ score", 
  ylab="Child test score",pch=20, xaxt="n", yaxt="n", type = "n")
curve (coef(fit.2)[1] + coef(fit.2)[2] + coef(fit.2)[3]*x, add=TRUE, col="gray")
curve (coef(fit.2)[1] + coef(fit.2)[3]*x, add=TRUE)
points (mom_iq[mom_hs==0], kid_score[mom_hs==0], pch=19)
points (mom_iq[mom_hs==1], kid_score[mom_hs==1], col="gray", pch=19)
```

## Coefficient interpretation:

* Predictive interpretation: how does the outcome differ on average when comparing *two groups* that differ by 1 in the relevant predictor (but are identical on all others). 
* Counterfactual interpretation: change *within individual* - a change of 1 in the predictor would *lead* to a change in the outcome.. This is easy to understand, but the implications of causality are a bit much, so some pretty strong assumptions are required here.. 

# Interactions

Allowing regression slopes to vary between groups - you can do that by including an interaction term. Then:

$kid.score = -11 + 51mom.hs + 1.1 * mom.iq -0.5 * mom.iq * mom.hs + error$

1.1 is the slope for kids of moms without hs, and the interaction term is the difference in slopes (0.6 is then the slope of the one for kids of moms with high school). Including interactions is a way to allow a model to be fit differently to different subsets of the data. 

```{r}
fit.3 <- lm(kid_score ~ mom_hs + mom_iq + mom_hs*mom_iq )
display(fit.3)
plot(mom_iq,kid_score, xlab="Mother IQ score", 
  ylab="Child test score",pch=20, xaxt="n", yaxt="n", col= mom_hs+1, xlim = c(0,150), ylim = c(-10,150))
curve(cbind(1,1,x,1*x) %*% coef(fit.3), col = 2, add = TRUE)
curve(cbind(1,0,x,0*x) %*% coef(fit.3), col = 1, add = TRUE)
points (mom_iq[mom_hs==0], kid_score[mom_hs==0], pch=19)
points (mom_iq[mom_hs==1], kid_score[mom_hs==1], col="gray", pch=19)
axis(1)
axis(2)
```

# Statistical inference

```{r}
fit.4 <- lm(kid_score ~ mom_hs + mom_iq)
display(fit.4)
```

* first we have the $\hat{\beta}$ coefficient estimates - these are a linear function of the outcome $y$
* then the standard errors of the coefficent estimates
* residuals $r_i = y_i - X_i \hat{\beta}$

## Residual standard deviation $\hat{\sigma}$ and explained variance

$\hat{\sigma} = \sqrt{\sum_{i=1}^n{r^2_i/(n-k)}}$ 

the residual standard deviation squared is the unexplained variance: $\hat{sigma}^2$, and if 
$s^2_y$ is the standard deviation of the outcome, then the **explained variance** is:

$R^2 = 1 - \frac{\hat{\sigma}^2}{s^2_y}$

SO important: because the explained variance depends on the standard deviation of the raw data, taking a subset (s is now smaller) will lead to R^2 also being smaller 

## Displaying uncertainty based on simulations

```{r}
fit.2 <- lm(kid_score ~ mom_iq)
fit.2.sim <- sim(fit.2) # arm function
plot(mom_iq, kid_score, col = "gray", pch = 19)
for(i in 1:10){
curve(fit.2.sim@coef[i,1] + fit.2.sim@coef[i,2]*x, add = TRUE, col = "gray")
}
curve(coef(fit.2)[1] + coef(fit.2)[2]*x, add = TRUE)
```

## Displaying one plot for each input variable

```{r}
fit.3 <- lm(kid_score~ mom_hs + mom_iq)
beta.hat <- coef(fit.3)
beta.sim <- sim(fit.3)@coef
par (mfrow = c(1,2))
plot(mom_iq, kid_score)
for(i in 1:10){
curve(cbind(1, mean(mom_hs), x) %*% beta.sim[i,], add = TRUE, col = "gray", lwd = 2)
}
curve(cbind(1, mean(mom_hs), x) %*% beta.hat, add = TRUE)
points(mom_iq, kid_score, pch = 19)

plot(jitter.binary(mom_hs), kid_score)
for(i in 1:10){
curve(cbind(1,  x, mean(mom_iq)) %*% beta.sim[i,], add = TRUE, col = "gray", lwd = 2)
}
curve(cbind(1, x, mean(mom_iq)) %*% beta.hat, add = TRUE)
points(jitter.binary(mom_hs), kid_score, pch = 19)

```


# Assumptions and diagnostics

## Validity
 Your data must map on to the question you are tyring to answer. Outcome measure should "accurately reflect the phenomoenon of interest", the model should include all relevant predictors, and model should generaliseze to all cases to which it will be applied. 
 
## Additivity and linearity 

the regression's 'deterministic component is a linear function of the separate predictors.' 
If additivity is violated then you migh need to transform the data or add interactions. If linearity is violated then a predictor mught be put as 1/x or log(x) instead. e.g. it is commont oo include both age and age^2  in health and medical regressison. This way a health measure can decline with age, and the rate becoming steeper as age increases. Another option is to include a nonlinear function such as a spline or other generalized additive model

## Independence of errors. 

Errors from the preditction line are independent!

## Equal variance of errors

## Normality of errors 

This is the least important one!

A good way to diagnose violations is to plot the residuals vs the figged balues, or somply the individual predictors. 
```{r}
fit.5 <- lm(kid_score ~ mom_iq)
par(mfrow = c(1,1))
plot(mom_iq, fit.5$residuals, ylab = "Residuals", pch = 19)
abline(h = c(1,-1)*sd( fit.5$residuals), lty = 2)
abline(h = 0)
```


# Prediciton and validation

```{r}
x.new <- data.frame (mom_hs = 1, mom_iq = 100)
predict(fit.3, x.new, interval = "prediction", level = 0.95)
```

# Exercises

## Exercise 3.1 

```{r}
www <- "http://www.stat.columbia.edu/%7Egelman/arm/examples/pyth/exercise2.1.dat"
mydata <- read.table(www, header = TRUE)
train <- mydata[!is.na(mydata$y),]
test <-  mydata[is.na(mydata$y),]
fit.ex1 <- lm(y ~ x1 + x2, data = train)
display(fit.ex1)
fit.ex1.sim <- sim(fit.ex1)

par(mfrow = c(1,2))

plot(train$x1, train$y)

for (i in 1:40){
curve(cbind(1,x,mean(train$x2)) %*% coef(fit.ex1.sim)[i,], add = TRUE, col = "gray")
}
curve(cbind(1,x,mean(train$x2)) %*% coef(fit.ex1), add = TRUE)

plot(train$x2, train$y)
for (i in 1:40){
curve(cbind(1,mean(train$x1), x) %*% coef(fit.ex1.sim)[i,], add = TRUE, col = "gray")
}
curve(cbind(1,mean(train$x1), x) %*% coef(fit.ex1), add = TRUE)

# residuals
par(mfrow = c(1,3))
plot(train$y, fit.ex1$residuals)
abline(h = c(1,-1)*sd(fit.ex1$residuals), lty = 2)
abline(h = 0)
plot(train$x1, fit.ex1$residuals)
abline(h = c(1,-1)*sd(fit.ex1$residuals), lty = 2)
abline(h = 0)

plot(train$x2, fit.ex1$residuals)
abline(h = c(1,-1)*sd(fit.ex1$residuals), lty = 2)
abline(h = 0)

# prediction:
predict(fit.ex1, test, interval = "prediction", level = 0.95)

```

## Exercise 3.2

log earnings predicted from log height

$ln(y) = b + a * ln(x)$

$y = e^b \times e^{a\times ln(x)}$

for every 1 percent increase in height there is a 0.8% increase in earnings

$1.008 \times y = e^b \times e^{a\times ln(1.01 \times x)} $

Divide one by the other:

$\frac{1.008 \times y }{ y } = \frac{e^b \times e^{a\times ln(1.01 \times x)}}{ e^b \times e^{a\times ln(x)}}$

$1.008 = \frac{e^{a\times ln(1.01 \times x)}}{e^{a\times ln(x)}}$

$1.008 = 1.01 ^a$

```{r}
a <- log(1.008)/log(1.01)
a 

b <- log(30000)-a*log(66)
b
```


## Exercise 3.3


```{r}
zscore <- rep(NA, 100)
for (k in 1 :100){
  var1 <- rnorm(1000, 0,1)
  var2 <- rnorm(1000, 0,1)
  fit <- lm(var2 ~ var1)
  zscore[k] <- coef(fit)[2]/se.coef(fit)[2]}
sum(zscore > 1.96)
```




## Exercise 3.4

```{r}
iq.data <- read.dta("../data/child.iq/child.iq.dta")

fit.1 <- lm(ppvt ~ momage, data = iq.data)
display(fit.1)
par(mfrow = c(1,1))
plot(iq.data$momage, iq.data$ppvt)
abline(fit.1)

fit.2 <- lm(ppvt ~ momage + educ_cat, data = iq.data)
display(fit.2)
par(mfrow = c(1,1))
plot(iq.data$momage, iq.data$ppvt)
for (i in 1:3) {
  curve(cbind(1, x, i) %*% coef(fit.2), add=TRUE, col=1:3)
}
```